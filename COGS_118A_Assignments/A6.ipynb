{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0fee8e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"A6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb4cd0",
   "metadata": {
    "id": "6bbb4cd0",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68f2f0ed9883b594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 6\n",
    "\n",
    "## **Due: Dec 5th (Friday), 2025, 11:59pm (Pacific Time)**\n",
    "\n",
    "### **Instructions:**\n",
    "\n",
    "Your Jupyter notebook assignment will often have 3 elements: written answers, code answers, and quiz answers. For written answers, you may insert images of your handwritten work in code cells, or write your answers in markdown and LaTeX. For quiz answers, your `record.txt` file will record your answer choices in the quiz modules for submission. Both your quiz answers and code answers will be autograded on Gradescope. This assignment does not have the quiz portion.\n",
    "\n",
    "For all elements, DO NOT MODIFY THE CELLS. Put your answers **only** in the answer cells given, and **do not delete cells**. If you fail to follow these instructions, you will lose points on your submission.\n",
    "\n",
    "You may use `print` to debug the output but remember to delete them as they may interfere with the autograder.\n",
    "\n",
    "Make sure to show the steps of your solution for every question to receive credit, not just the final answer. You may search information online but you will need to write code/find solutions to answer the questions yourself. You will submit your .ipynb file and record.txt to gradescope when you are finished.\n",
    "\n",
    "### **Late Policy:**\n",
    "\n",
    "5% reduction for the first day and 10% reduction afterwards for every extra day past due.\n",
    "\n",
    "### How to Include Your Math Written Answer?\n",
    "\n",
    "You could use markdowns' include image functionality (recommended) or $\\LaTeX$ in markdown to submit your written responses.\n",
    "\n",
    "#### Include Images (recommended)\n",
    "If you are still getting familiar with using LaTeX, handwrite the response on paper or the stylus. Take a picture or screenshot of your answer, and include that image in the Jupyter Notebook. Be sure to include that image in the `imgs` directory. Let's say you have your Q1 response saved as `imgs/Q1.png`; the markdown syntax to include that image is `![Q1](imgs/Q1.png)`. Please use simple alphanumeric file names and avoid blank spaces, underscores, etc.\n",
    "\n",
    "#### $\\LaTeX$ (only if you're familiar with KaTeX or similar tools)\n",
    "[Here is a tutorial about using $\\LaTeX$ in Jupyter Notebook](https://patrickwalls.github.io/mathematicalpython/jupyter/latex/). You could also find various $\\LaTeX$ tutorials and cheat sheets online.\n",
    "\n",
    "## Important Notice\n",
    "\n",
    "You must check both submission output on the gradescope (`Assignment 6 - Notebook` and `Assignment 6 - Manual Grading`) correctly reflects your work and responses. If you notice inconsistencies between your notebook and the Manual Grading portion, you need to make a Piazza post, and we can help you with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8676f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "49d8676f"
   },
   "source": [
    "## Question 1 Implement the Linear SVM\n",
    "\n",
    "\n",
    "Assume in a binary classification problem, we need to predict a label $y\\in \\{-1,+1\\}$ for a feature vector $\\mathbf{x}=[x_0,x_1]^\\top$. In this section, we will learn a linear SVM classifier to solve this binary classification problem. The decision rule of the linear SVM classifier is defined as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}; \\mathbf{w}, b)=\\begin{cases}\n",
    "+1, & \\text{if }\\mathbf{w}^T\\mathbf{x}+b\\geq 0, \\\\\n",
    "-1, & \\text{otherwise}. \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}=[w_0,w_1]^\\top$ is the weight vector, and $b$ is the bias scalar. Given a training dataset $S_\\text{training} = \\{(\\mathbf{x}_i, y_i)\\}, i=1,\\ldots,n\\}$, we wish to optimize the SVM loss $\\mathcal{L}(\\mathbf w, b)$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\textbf{w}, b) = \\frac{1}{2}||\\textbf{w}||_2^2+C\\sum_{i=1}^{n} \\big(1-y_{i} (\\mathbf{w}^T \\mathbf{x}_{i} + b)\\big)_{+}\n",
    "$$\n",
    "\n",
    "where $(z)_+ = \\max(0, z)$ is called the rectifier function and $C$ is a scalar constant.\n",
    "\n",
    "\n",
    "In this problem, we attempt to obtain the optimal parameters $\\mathbf{w}^*$ and $b^*$ by using a standard gradient descent algorithm. Assume $a_i = 1-y_i(\\mathbf{w}^T \\mathbf{x}_i + b)$, the gradient for $\\mathbf w$ and the gradient for $b$ are shown as following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}} = \\mathbf{w}-C\\sum_{i=1}^n \\mathbb{1}(a_i > 0) y_i\\mathbf{x}_i, \\quad\\quad\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b} = -C\\sum_{i=1}^n \\mathbb{1}(a_i > 0) y_i.\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1}(z>0)$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbb{1}(z>0)=\\begin{cases}\n",
    "1, & \\text{if } z > 0, \\\\\n",
    "0, & \\text{if } z \\leq 0. \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In reality, we typically tackle this problem in a matrix form: First, we represent data points as matrices $X=[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n]^T$ and $Y=[y_1, y_2, \\ldots, y_n]^T$. Thus, the SVM loss $\\mathcal{L}(\\mathbf{w},b)$ can be formulated as:\n",
    "$$\n",
    "\\mathbf{a} = \\mathbf{1} - Y \\circ (X\\mathbf{w} + b\\mathbf{1})),  \\quad\\quad \\mathcal{L}(\\mathbf w, b)=\\frac{1}{2}\\mathbf{w}^T \\mathbf{w} + C\\mathbf{1}^T (\\mathbf{a})_+\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1} = [1,1,...,1]^T \\in \\mathbb{R}^n$ is a $n$-dimensional column vector, $\\mathbf{a}=[a_1,a_2,...,a_n]^T\\;\\in \\mathbb{R}^n$ is a $n$-dimensional column vector, $(\\cdot)_+$ is the element-wise rectifier function, and \"$\\circ$\" is an element-wise product operator. Similarly, we can have the gradient for $\\mathbf{w}$ and $b$ in the matrix form:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}} = \\mathbf{w} - C X^T (\\mathbb{1}(\\mathbf{a} > \\mathbf{0}) \\circ Y), ~~~\n",
    "\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b} = - C\\mathbf{1}^T(\\mathbb{1}(\\mathbf{a} > \\mathbf{0}) \\circ Y)\n",
    "$$\n",
    "where $\\mathbb{1}(\\mathbf{a}>\\mathbf{0}) = [\\mathbb{1}(a_1 > 0), \\mathbb{1}(a_2 > 0), ..., \\mathbb{1}(a_n > 0)]^T$.\n",
    "\n",
    "After obtaining the linear SVM model with the optimal $\\mathbf{w}^*, b^*$ from gradient descent, we can use the decision rule to predict the label $f(\\mathbf{x}; \\mathbf{w}^*, b^*)\\in \\{-1,+1\\}$ of the feature vector $\\mathbf{x}$. The error $e_\\text{training}$ on the training set $S_\\text{training}=\\{(\\mathbf{x}_i,y_i)\\}$ is defined as:\n",
    "$$\n",
    "e_\\text{training} = \\frac{1}{n}\\sum_{i=1}^n\\mathbb{1}\\big(y_i \\neq f(\\mathbf{x}_i;\\mathbf{w}^*, b^*)\\big)\n",
    "$$\n",
    "\n",
    "and we can define the test error $e_\\text{test}$ on the test set $S_\\text{test}$ in the same way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f04c68",
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1701712569178,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "95f04c68"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9af4ba",
   "metadata": {
    "id": "8e9af4ba"
   },
   "source": [
    "### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e43f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1701712569468,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "0c1e43f3",
    "outputId": "fa2acbc1-5c05-43ab-ac9f-8062e3d5dff7"
   },
   "outputs": [],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "# Here for convenience, we divide the 3 kinds of flowers into 2 groups:\n",
    "#     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "#     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "# Thus we use (iris.target > 1.5) to divide the targets into 2 groups.\n",
    "# This line of code will assign:\n",
    "#    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "#    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "Y = (iris.target > 1.5).reshape(-1,1).astype(float) # The shape of Y is (150, 1), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value.\n",
    "Y[Y==0] = -1\n",
    "\n",
    "X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_and_Y[0])               # The result should be always: [ 5.8  4.   1.2  0.2  -1. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493de8db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1701712569470,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "493de8db",
    "outputId": "fb61e6ac-08b3-4061-ab93-219c6792e6fc"
   },
   "outputs": [],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:4]\n",
    "Y_shuffled = X_and_Y[:,4]\n",
    "\n",
    "\n",
    "X_train = X_shuffled[:100][:,[3,1]] # Shape: (100,2)\n",
    "X_train = np.delete(X_train, 42, axis=0) # Remove a point for separability.\n",
    "Y_train = Y_shuffled[:100]          # Shape: (100,)\n",
    "Y_train = np.delete(Y_train, 42, axis=0) # Remove a point for separability.\n",
    "X_test = X_shuffled[100:][:,[3,1]]  # Shape: (50,2)\n",
    "Y_test = Y_shuffled[100:]           # Shape: (50,)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c6b5f",
   "metadata": {
    "id": "525c6b5f"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d86888",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1701712569471,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "86d86888"
   },
   "outputs": [],
   "source": [
    "def vis(X, Y, W=None, b=None):\n",
    "    indices_neg1 = (Y == -1).nonzero()[0]\n",
    "    indices_pos1 = (Y == 1).nonzero()[0]\n",
    "    plt.scatter(X[:,0][indices_neg1], X[:,1][indices_neg1],\n",
    "                c='blue', label='class -1')\n",
    "    plt.scatter(X[:,0][indices_pos1], X[:,1][indices_pos1],\n",
    "                c='red', label='class 1')\n",
    "    plt.legend()\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "\n",
    "    if W is not None:\n",
    "        # w0x0+w1x1+b=0 => x1=-w0x0/w1-b/w1\n",
    "        w0 = W[0]\n",
    "        w1 = W[1]\n",
    "        temp = -w1*np.array([X[:,1].min(), X[:,1].max()])/w0-b/w0\n",
    "        x0_min = max(temp.min(), X[:,0].min())\n",
    "        x0_max = min(temp.max(), X[:,1].max())\n",
    "        x0 = np.linspace(x0_min,x0_max,100)\n",
    "        x1 = -w0*x0/w1-b/w1\n",
    "        plt.plot(x0,x1,color='black')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0445e68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1701712569834,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "c0445e68",
    "outputId": "0324f7c3-2fb0-47c8-f8f4-1af8e649fe6c"
   },
   "outputs": [],
   "source": [
    "# Visualize training set.\n",
    "vis(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c76a58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "78c76a58"
   },
   "source": [
    "### Linear SVM Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2719ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8c2719ee"
   },
   "source": [
    "In this problem, we would like to use the gradient descent to calculate the parameters $\\mathbf{w},b$ for a linear SVM model.\n",
    "If we have the loss function $\\mathcal{L}(\\mathbf{w},b)$, then a typical gradient descent algorithm contains the following steps:\n",
    "\n",
    "**Step 1**. Initialize the parameters $\\mathbf{w}$, $b$.\n",
    "\n",
    "for i = 1 to #iterations:\n",
    "\n",
    "- **Step 2**. Compute the partial derivatives $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}}$, $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$.\n",
    "\n",
    "- **Step 3**. Update the parameters\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial \\mathbf{w}}, \\quad\\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Note that in the code, we use `W` and `b` to represent the weight vector $\\mathbf{w}$ and bias scalar $b$.In this problem, we would like to use the gradient descent to calculate the parameters $\\mathbf{w},b$ for a linear SVM model.\n",
    "If we have the loss function $\\mathcal{L}(\\mathbf{w},b)$, then a typical gradient descent algorithm contains the following steps:\n",
    "\n",
    "**Step 1**. Initialize the parameters $\\mathbf{w}$, $b$.\n",
    "\n",
    "for i = 1 to #iterations:\n",
    "\n",
    "- **Step 2**. Compute the partial derivatives $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}}$, $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$.\n",
    "\n",
    "- **Step 3**. Update the parameters\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial \\mathbf{w}}, \\quad\\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Note that in the code, we use `W` and `b` to represent the weight vector $\\mathbf{w}$ and bias scalar $b$.\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6640c3",
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1701712561250,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "7e6640c3",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Judge function: 1(a != b). It supports scalar, vector and matrix.\n",
    "def judge_1(a, b):\n",
    "    return np.array(a != b).astype(np.float32)\n",
    "\n",
    "# Judge function: 1(z > 0). It supports scalar, vector and matrix.\n",
    "def judge_2(z):\n",
    "    return np.array(z > 0).astype(np.float32)\n",
    "\n",
    "# Rectifier function: (z)_+ = max(0, z). It supports scalar, vector and matrix.\n",
    "def rectifier(z):\n",
    "    return np.clip(z, a_min=0, a_max=None)\n",
    "\n",
    "# Linear SVM classifier.\n",
    "def f_linear_svm(x, W, b):\n",
    "    # x should be a 2-dimensional vector,\n",
    "    # W should be a 2-dimensional vector,\n",
    "    # b should be a scalar.\n",
    "    # you should return a scalar which is -1 or 1.\n",
    "    ...\n",
    "\n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, W, b):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5176d29",
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1701712574500,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "d5176d29",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W, b) with respect to W and b.\n",
    "def grad_L_W_b(X, Y, W, b, C):\n",
    "    ...\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z6QaAKffFojG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1701714516653,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "Z6QaAKffFojG",
    "outputId": "cfdf1170-8a74-4f69-e68f-b861371508cd",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "grad_L_W_b(X_train, Y_train, [2, 3], 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f9321",
   "metadata": {
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1701712619717,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "e68f9321",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Loss L(W, b).\n",
    "def L_W_b(X, Y, W, b, C):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YYy4ijC_-UbL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1701712621179,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "YYy4ijC_-UbL",
    "outputId": "c13c1b05-2aaa-4e9e-d704-b3012c386044",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "L_W_b(X_train, Y_train, [2, 3], 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898e622",
   "metadata": {
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1701713894918,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "8898e622",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Some settings.\n",
    "learning_rate = 0.0001\n",
    "iterations    = 10000\n",
    "losses = []\n",
    "\n",
    "# Gradient descent algorithm for linear SVM classifier.\n",
    "# Step 1. Initialize the parameters W, b.\n",
    "W = np.zeros(2)\n",
    "b = 0\n",
    "C = 1000\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Step 2. Compute the partial derivatives.\n",
    "    grad_W, grad_b = grad_L_W_b(X_train, Y_train, W, b, C)\n",
    "    # Step 3. Update the parameters.\n",
    "    W = ...\n",
    "    b = ...\n",
    "\n",
    "    # Track the training losses.\n",
    "    losses.append(L_W_b(X_train, Y_train, W, b, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SHDh-FThDWxO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1701713901567,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "SHDh-FThDWxO",
    "outputId": "0a44860e-531f-4d7e-d8dc-057469a602be",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "losses[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef948c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Q1_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0e7ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "24a0e7ec"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Visualize the results\n",
    "\n",
    "Please complete the following codes to visualize the decision boundary of the perceptron model. You may use the `vis` function defined above.\n",
    "\n",
    "Also, please plot the training error curve with respect to the number of iterations.\n",
    "\n",
    "You should only insert your code in the `...` part.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e78c76",
   "metadata": {
    "id": "18e78c76",
    "outputId": "cd4dd9a6-93cd-4034-8d76-c7e0d9676198",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Show decision boundary, training error and test error.\n",
    "print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(W[0],W[1],b))\n",
    "...\n",
    "print('Training error: {}'.format(calc_error(X_train, Y_train, W, b)))\n",
    "...\n",
    "print('Test error: {}'.format(calc_error(X_test, Y_test, W, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246f050",
   "metadata": {
    "id": "3246f050",
    "outputId": "9ed11d92-5e8f-40ce-819d-5593920c5cc2",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot training loss curve.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff4394",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5aff4394"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 2 SVM with Scikit-Learn\n",
    "\n",
    "In this problem, you are required to solve the same binary classification problem from Question 1. You are allowed to use the functions from `scikit-learn` in this question.\n",
    "\n",
    "\n",
    "Load the training set $S_\\text{training}$ and the test set $S_\\text{test}$: In the code, we use `X_train` and `Y_train` to represent the feature vectors and the labels of the training set $S_\\text{training}$ respectively. Similarly, we use `X_test` and `Y_test` for the test set $S_\\text{test}$.\n",
    "\n",
    "Train the SVM classifier using a linear kernel: In the SVM, there is a hyper-parameter $C$. We train the linear SVM classifier on the training set $S_\\text{training}$ with each $C$ from the list below and calculate its training error $e_\\text{training}$:\n",
    "$$C \\in \\{0.1, 1, 10, 100, 1000\\}$$\n",
    "\n",
    "We aim to obtain the best hyper-parameter $C^*$ corresponding to the minimum **training error** $e_\\text{training}^*$ among all $C$s listed above.\n",
    "\n",
    "Use the trained classifier corresponding to the best hyper-parameter $C^*$ to calculate the test error $e_\\text{test}$ on test set $S_\\text{test}$.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0cc4c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1596,
     "status": "ok",
     "timestamp": 1701724773406,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "9da0cc4c"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7b64f",
   "metadata": {
    "id": "6cd7b64f"
   },
   "source": [
    "### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8afdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1701724787045,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "7db8afdd",
    "outputId": "43e5db6b-e8f2-42a2-b26b-6996319aaee5"
   },
   "outputs": [],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "# Here for convenience, we divide the 3 kinds of flowers into 2 groups:\n",
    "#     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "#     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "# Thus we use (iris.target > 1.5) to divide the targets into 2 groups.\n",
    "# This line of code will assign:\n",
    "#    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "#    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "Y = (iris.target > 1.5).reshape(-1,1).astype(float) # The shape of Y is (150, 1), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value.\n",
    "Y[Y==0] = -1\n",
    "\n",
    "X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_and_Y[0])               # The result should be always: [ 5.8  4.   1.2  0.2  -1. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c84e50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1701724790349,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "60c84e50",
    "outputId": "0c57d5ef-d99c-4339-af6c-e864ae7f69e1"
   },
   "outputs": [],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:4]\n",
    "Y_shuffled = X_and_Y[:,4]\n",
    "\n",
    "\n",
    "X_train = X_shuffled[:100][:,[3,1]] # Shape: (100,2)\n",
    "X_train = np.delete(X_train, 42, axis=0) # Remove a point for separability.\n",
    "Y_train = Y_shuffled[:100]          # Shape: (100,)\n",
    "Y_train = np.delete(Y_train, 42, axis=0) # Remove a point for separability.\n",
    "X_test = X_shuffled[100:][:,[3,1]]  # Shape: (50,2)\n",
    "Y_test = Y_shuffled[100:]           # Shape: (50,)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31d872",
   "metadata": {
    "id": "2c31d872"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faec24d",
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1701724793008,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "9faec24d"
   },
   "outputs": [],
   "source": [
    "def vis(X, Y, W=None, b=None):\n",
    "    indices_neg1 = (Y == -1).nonzero()[0]\n",
    "    indices_pos1 = (Y == 1).nonzero()[0]\n",
    "    plt.scatter(X[:,0][indices_neg1], X[:,1][indices_neg1],\n",
    "                c='blue', label='class -1')\n",
    "    plt.scatter(X[:,0][indices_pos1], X[:,1][indices_pos1],\n",
    "                c='red', label='class 1')\n",
    "    plt.legend()\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "\n",
    "    if W is not None:\n",
    "        # w0x0+w1x1+b=0 => x1=-w0x0/w1-b/w1\n",
    "        w0 = W[0]\n",
    "        w1 = W[1]\n",
    "        temp = -w1*np.array([X[:,1].min(), X[:,1].max()])/w0-b/w0\n",
    "        x0_min = max(temp.min(), X[:,0].min())\n",
    "        x0_max = min(temp.max(), X[:,1].max())\n",
    "        x0 = np.linspace(x0_min,x0_max,100)\n",
    "        x1 = -w0*x0/w1-b/w1\n",
    "        plt.plot(x0,x1,color='black')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d202c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1701724796211,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "ea7d202c",
    "outputId": "b9b4fec4-fb42-41e9-cb17-05ffd95fecf3"
   },
   "outputs": [],
   "source": [
    "# Visualize training set.\n",
    "vis(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cd69e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "532cd69e"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Linear SVM Using Scikit-Learn\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b794d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1701724799069,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "c9b794d8",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, classifier):\n",
    "\n",
    "    # Hint: Use classifier.predict()\n",
    "    Y_pred = ...\n",
    "\n",
    "    # Hint: Use accuracy_score().\n",
    "    e = ...\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cc710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5935,
     "status": "ok",
     "timestamp": 1701724864682,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "e60cc710",
    "outputId": "6ed1232f-b314-4841-866b-d7e98bbb7c9e",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "C_list = [0.1, 1, 10, 100, 1000]\n",
    "opt_e_training = 1.0   # Optimal training error.\n",
    "opt_classifier = None  # Optimal classifier.\n",
    "opt_C          = None  # Optimal C.\n",
    "\n",
    "for C in C_list:\n",
    "    # Create a linear SVM classifier.\n",
    "    # Hints: You can use svm.LinearSVC()\n",
    "    #        Besides, we use Hinge loss and L2 penalty for weights.\n",
    "    #        The max iterations should be set to 100000.\n",
    "    #        The regularization parameter should be set as C.\n",
    "    #        The other arguments of svm.LinearSVC() are set as default values.\n",
    "    classifier = ...\n",
    "\n",
    "    # Use the classifier to fit the training set (use X_train, Y_train).\n",
    "    # Hint: You can use classifier.fit().\n",
    "\n",
    "    ...\n",
    "\n",
    "    # Obtain the weights and bias from the linear SVM classifier.\n",
    "    W = classifier.coef_[0]\n",
    "    b = classifier.intercept_[0]\n",
    "\n",
    "    # Show decision boundary, training error and test error.\n",
    "    print('C = {}'.format(C))\n",
    "    print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(W[0],W[1],b))\n",
    "    vis(X_train, Y_train, W, b)\n",
    "    e_training = calc_error(X_train, Y_train, classifier)\n",
    "    print('Training error: {}'.format(e_training))\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "    # Judge if it is the optimal one.\n",
    "    if e_training < opt_e_training:\n",
    "        opt_e_training = e_training\n",
    "        opt_classifier = classifier\n",
    "        opt_C = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d98ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1701724882702,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "761d98ed",
    "outputId": "f2f6d12f-b6c9-4249-b895-10cd42e759c1",
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Obtain the weights and bias from the best linear SVM classifier.\n",
    "opt_W = opt_classifier.coef_[0]\n",
    "opt_b = opt_classifier.intercept_[0]\n",
    "print('Best parameter C* = {}'.format(opt_C))\n",
    "print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(opt_W[0],opt_W[1],opt_b))\n",
    "vis(X_test, Y_test, opt_W, opt_b)\n",
    "print('Test error: {}'.format(calc_error(X_test, Y_test, opt_classifier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c1663",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f97c1663"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 3 Cross Validation\n",
    "\n",
    "Given a training dataset $S_\\text{training} = \\{(x_i, y_i)\\}, i=1,\\ldots,6\\}$ where $x_i \\in \\mathbb{R}$ is the feature scalar and $y_i \\in \\{-1, +1\\}$ is the corresponding label. The data points in the dataset $S_\\text{training}$ are given below:\n",
    "$$(x_1, y_1) = (2, -1), \\quad (x_2, y_2) = (7, -1), \\quad (x_3, y_3) = (4, +1),$$\n",
    "$$(x_4, y_4) = (1, -1), \\quad (x_5, y_5) = (3, +1), \\quad (x_6, y_6) = (6, +1).$$\n",
    "\n",
    "Suppose you are training a linear classifier $f(x;a,b) = \\text{sign}(ax + b)$ with 2-fold cross-validation where $\\text{sign}(z)$ is defined as:\n",
    "$$\n",
    "\\text{sign}(z) = \\left\\{\n",
    "             \\begin{array}{cl}\n",
    "              1,  & z \\geq 0 \\\\\n",
    "             -1,  & z < 0.\n",
    "             \\end{array}  \n",
    "        \\right. \\nonumber\n",
    "$$\n",
    "\n",
    "- You have split the dataset $S_\\text{training}$ into:\n",
    "$$S_1 = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}$$\n",
    "$$S_2 = \\{(x_4, y_4), (x_5, y_5), (x_6, y_6)\\}$$\n",
    "\n",
    "- After training the classifier $f(x;a,b)$ on $S_1$, you have obtained the parameters $a_1 = -1, b_1 = 5$ and then try to validate the classifier on $S_2$.\n",
    "\n",
    "- After training the classifier $f(x;a,b)$ on $S_2$, you have obtained the parameters $a_2 = 2, b_2 = -3$ and then try to validate the classifier on $S_1$.\n",
    "\n",
    "\n",
    "Please finish the tasks below:\n",
    "\n",
    "1. Calculate the **average training error** in the 2-fold cross-validation.\n",
    "    \n",
    "    **Note:** The definition of average training error is the mean of the error of classifier $f(x; a_1, b_1)$ on $S_1$ and the error of classifier $f(x; a_2, b_2)$ on $S_2$.\n",
    "    \n",
    "2. Calculate the **average validation error** (i.e. the cross-validation error) in the 2-fold cross-validation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72a40f-3a0f-4959-a910-117e96d4f231",
   "metadata": {
    "id": "eb72a40f-3a0f-4959-a910-117e96d4f231",
    "otter": {
     "tests": [
      "Q3_1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "AVG_TRAINING_ERROR = ...\n",
    "AVG_VAL_ERROR = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518da14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9518da14"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Question 4 SVM with the RBF Kernel (Bonus)\n",
    "\n",
    "In this problem, you are required to solve the same binary classification problem from Question 3 using `scikit-learn`. However, you need to use the SVM with the radial basis function (RBF) kernel to conduct the binary classification in this question.\n",
    "\n",
    "- Load the training set $S_\\text{training}$ and the test set $S_\\text{test}$: In the code, we use `X_train` and `Y_train` to represent the feature vectors and the labels of the training set $S_\\text{training}$ respectively. Similarly, we use `X_test` and `Y_test` for the test set $S_\\text{test}$.\n",
    "\n",
    "- Train the SVM classifier using a RBF kernel: For the SVM with the RBF kernel, there are two hyper-parameters, $C$ and $\\gamma$, to control the model. We train the SVM classifier on the training set $S_\\text{training}$ with each combination of $C$ and $\\gamma$ from the list below and calculate its training error $e_\\text{training}$:\n",
    "$$C \\in \\{1, 10, 100, 1000, 10000\\}, \\quad\\quad \\gamma \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\\}$$\n",
    "\n",
    "We aim to obtain the best hyper-parameters $(C^*, \\gamma^*)$ corresponding to the minimum \\textbf{training error} $e_\\text{training}^*$ among all combinations of $C\\text{s}$ and $\\gamma\\text{s}$ listed above.\n",
    "\n",
    "- Use the trained classifier corresponding to the best hyper-parameters $(C^*, \\gamma^*)$ to calculate the test error $e_\\text{test}$ on test set $S_\\text{test}$.   \n",
    "\n",
    "Please write the code and provide a heatmap of training errors corresponding to all combinations of $(C, \\gamma)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0963b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1701728373627,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "fa0963b2"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085df319",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "085df319"
   },
   "source": [
    "### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181510d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701728374084,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "181510d3",
    "outputId": "15708d6d-9b35-419a-b677-c1e487f95adb"
   },
   "outputs": [],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "# Here for convenience, we divide the 3 kinds of flowers into 2 groups:\n",
    "#     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "#     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "# Thus we use (iris.target > 1.5) to divide the targets into 2 groups.\n",
    "# This line of code will assign:\n",
    "#    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "#    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "Y = (iris.target > 1.5).reshape(-1,1).astype(float) # The shape of Y is (150, 1), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value.\n",
    "Y[Y==0] = -1\n",
    "\n",
    "X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_and_Y[0])               # The result should be always: [ 5.8  4.   1.2  0.2  -1. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a57c3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701728376450,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "b6a57c3e",
    "outputId": "64b2f510-a6ca-4c88-8e21-9cfdf557fe41"
   },
   "outputs": [],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:4]\n",
    "Y_shuffled = X_and_Y[:,4]\n",
    "\n",
    "\n",
    "X_train = X_shuffled[:100][:,[3,1]] # Shape: (100,2)\n",
    "X_train = np.delete(X_train, 42, axis=0) # Remove a point for separability.\n",
    "Y_train = Y_shuffled[:100]          # Shape: (100,)\n",
    "Y_train = np.delete(Y_train, 42, axis=0) # Remove a point for separability.\n",
    "X_test = X_shuffled[100:][:,[3,1]]  # Shape: (50,2)\n",
    "Y_test = Y_shuffled[100:]           # Shape: (50,)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98653c3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "98653c3b"
   },
   "source": [
    "### SVM with RBF Using Scikit-Learn\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182fa65",
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1701728378983,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "5182fa65",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, classifier):\n",
    "    Y_pred = classifier.predict(X)\n",
    "    e = 1 - accuracy_score(Y, Y_pred)\n",
    "    return e\n",
    "\n",
    "\n",
    "# Draw the heatmap of training errors.\n",
    "def draw_heatmap(training_errors, gamma_list, C_list):\n",
    "    # training_errors: A NumPy array with the shape (len(C_list), len(gamma_list))\n",
    "    # gamma_list: List of gamma(s).\n",
    "    # C_list: List of C(s).\n",
    "    plt.figure(figsize = (5,4))\n",
    "    ax = sns.heatmap(training_errors, annot=True, fmt='.3f',\n",
    "                     xticklabels=gamma_list, yticklabels=C_list)\n",
    "    ax.collections[0].colorbar.set_label(\"error\")\n",
    "    ax.set(xlabel = r'$\\gamma$', ylabel=r'$C$')\n",
    "    plt.title(r'Training error w.r.t $C$ and $\\gamma$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26b108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1701728381336,
     "user": {
      "displayName": "Lucy Lennemann",
      "userId": "00186712551291789975"
     },
     "user_tz": 480
    },
    "id": "ac26b108",
    "outputId": "25ab19fb-eb31-44ce-90a7-e0c92a907d82",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "C_list = [1, 10, 100, 1000, 10000]\n",
    "gamma_list = [1e-6, 1e-5, 1e-4, 1e-3,1e-2]\n",
    "\n",
    "# An example of using draw_heatmap().\n",
    "#    errors = np.random.random((len(C_list), len(gamma_list)))\n",
    "#    draw_heatmap(errors, gamma_list, C_list)\n",
    "#  ......  Hint: You may use svm.SVC().\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a996f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5b2a996f"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 5 K Nearest Neighbors (Bonus)\n",
    "\n",
    "In this problem, you need to implement the $k$ nearest neighbors ($k$-NN) algorithm and apply it to the binary classification. Here we use the modified Iris dataset $S=\\{(\\mathbf{x}_i, y_i)\\}$ where each feature vector $\\mathbf{x} \\in \\mathbb{R}^2$ and label $y \\in \\{-1, +1\\}$. You are **not** allowed to use `sklearn.neighbors.KNeighborsClassifier()` in your code, but you can use it to validate your implementation.\n",
    "\n",
    "\n",
    "- Load the modified Iris dataset. The dataset $S$ is split to three subsets: The training set $S_\\text{training}$, the validation set $S_\\text{validation}$ and the test set $S_\\text{test}$. In the code, we use `X_train`, `Y_train` for the feature vectors and labels of the training set respectively. Similar notations are also used for the validation and the test sets.\n",
    "\n",
    "- Implement $k$-NN algorithm in 3 steps.\n",
    "\n",
    "    1. For each feature vector $\\mathbf{x}$ you are predicting a label, you need to calculate the distances between this feature vector $\\mathbf{x}$ and all the feature vectors in the training set $S_\\text{training}$.\n",
    "    \n",
    "    2. Then sort all distances in ascending order and pick the labels for the $k$ minimum distances.\n",
    "    \n",
    "    3. Count the number of negative labels $N_{y=-1}$, and the number of the positive labels $N_{y=+1}$ from $k$ labels picked in step 2. Use the following decision rule to predict label $\\hat{y}$ for each feature vector $\\mathbf{x}$:\n",
    "    \n",
    "    $$\n",
    "        \\hat{y} = \\left\\{\n",
    "                     \\begin{array}{cl}\n",
    "                      +1,  & N_{y=-1} < N_{y=+1}, \\\\\n",
    "                     -1,  & N_{y=-1} \\geq N_{y=+1}.\n",
    "                     \\end{array}  \n",
    "                \\right. \\nonumber\n",
    "    $$\n",
    "\n",
    "\n",
    "Here we assume **Euclidean distance** as the distance metric. For more details, please refer to the code and the corresponding part in the slides.\n",
    "\n",
    "- Use the validation set to obtain optimal $k^*$. In $k$-NN, there is a hyper-parameter $k$ which adjusts the number of nearest neighbors. You would need to perform a grid search on the following list of $k$:\n",
    "$$k \\in \\{1,2,3\\}$$\n",
    "\n",
    "For each $k$, you need to form a $k$-NN classifier with the training set $S_\\text{training}$. Then, use the classifier to make predictions on the validation set $S_\\text{validation}$ and calculate the error $e_\\text{validation}$. We aim to obtain the best hyper-parameter $k^*$ corresponding to the **minimum validation error** $e_\\text{validation}^*$ among all $k\\text{s}$.\n",
    "\n",
    "- Use the obtained classifier corresponding to the best hyper-parameter $k^*$ to calculate the test error $e_\\text{test}$ on test set $S_\\text{test}$.   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563b28d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1563b28d"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import scipy\n",
    "from matplotlib.colors import ListedColormap\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff3e7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "26ff3e7b"
   },
   "source": [
    "### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7456861",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c7456861",
    "outputId": "4bbde9e4-c78e-4e50-bcb8-3e426a6638bc"
   },
   "outputs": [],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "Y = (iris.target > 1.5).reshape(-1,1).astype(float) # The shape of Y is (150, 1), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value.\n",
    "Y[Y==0] = -1                    # Convert labels from {0, 1} to {-1, 1}\n",
    "\n",
    "X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_and_Y[0])               # Should be: [5.8 4.  1.2 0.2 -1. ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c671c4e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8c671c4e",
    "outputId": "cf73ae4a-b2f5-48cf-a11d-415e51fc8d14"
   },
   "outputs": [],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:4]\n",
    "Y_shuffled = X_and_Y[:,4]\n",
    "\n",
    "X_train = X_shuffled[:50][:, [3,1]]     # Shape: (50,2)\n",
    "Y_train = Y_shuffled[:50]               # Shape: (50,)\n",
    "X_val   = X_shuffled[50:100][:, [3,1]]  # Shape: (50,2)\n",
    "Y_val   = Y_shuffled[50:100]            # Shape: (50,)\n",
    "X_test  = X_shuffled[100:][:, [3,1]]    # Shape: (50,2)\n",
    "Y_test  = Y_shuffled[100:]              # Shape: (50,)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03461a17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "03461a17"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec2938",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a2ec2938"
   },
   "outputs": [],
   "source": [
    "def vis(X, Y, knn_classifier=None):\n",
    "    # Visualize k-NN.\n",
    "    if knn_classifier is not None:\n",
    "        # Calculate min, max and create grids.\n",
    "        h = .02\n",
    "        x0_min, x0_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "        x1_min, x1_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "        x0s, x1s = np.meshgrid(np.arange(x0_min, x0_max, h),\n",
    "                               np.arange(x1_min, x1_max, h))\n",
    "        xs = np.stack([x0s, x1s], axis=-1).reshape(-1, 2)\n",
    "\n",
    "        # Predict class using kNN classifier and data.\n",
    "        ys_pred = np.array([knn_classifier(x) for x in xs])\n",
    "        ys_pred = ys_pred.reshape(x0s.shape)\n",
    "\n",
    "        # Put the result into a color plot.\n",
    "        # Color map: #00AAFF - blue, #FFAAAA - red, #AAFFAA - green\n",
    "\n",
    "        cmap_light = ListedColormap(['#00AAFF', '#FFAAAA'])\n",
    "        plt.pcolormesh(x0s, x1s, ys_pred, cmap=cmap_light, alpha=0.3)\n",
    "\n",
    "    indices_neg1 = (Y == -1).nonzero()[0]\n",
    "    indices_pos1 = (Y == 1).nonzero()[0]\n",
    "    plt.scatter(X[:,0][indices_neg1], X[:,1][indices_neg1],\n",
    "                c='blue', label='class -1', alpha=0.3)\n",
    "    plt.scatter(X[:,0][indices_pos1], X[:,1][indices_pos1],\n",
    "                c='red', label='class +1', alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79052d75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "79052d75",
    "outputId": "97cb7849-3b10-41d5-9aa7-81616be8c806"
   },
   "outputs": [],
   "source": [
    "# Visualize training set.\n",
    "vis(X_train, Y_train)\n",
    "# Note that some points have darker color since there can be\n",
    "# multiple points at the same location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764b3ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4764b3ff"
   },
   "source": [
    "### k Nearest Neighbors\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09c728",
   "metadata": {
    "id": "4c09c728",
    "otter": {
     "tests": [
      "Q5_1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Euclidean distance.\n",
    "def calc_distance(x1, x2):\n",
    "    # x1, x2 are two NumPy vectors\n",
    "    # Return the Euclidean distance between x1 and x2. It should be a scalar.\n",
    "    dist = ...\n",
    "    return dist\n",
    "\n",
    "# k nearest neighbor predictor.\n",
    "def f_knn(x, X_train, Y_train, k):\n",
    "    # Create the list of (distance, label) pairs.\n",
    "    dist_label_pairs = []\n",
    "    for xi, yi in zip(X_train, Y_train):\n",
    "        # Calculate the distance.\n",
    "        dist = calc_distance(xi, x)\n",
    "        # Add a (distance, label) pair to the list.\n",
    "        dist_label_pairs.append((dist, yi))\n",
    "    # Sort the pairs by distance (ascending).\n",
    "    sorted_dist_label_pairs = sorted(dist_label_pairs, key=lambda x:x[0])\n",
    "    # Obtain the first k pairs (corresponding to k smallest distances).\n",
    "    k_dist_label_pairs = ...\n",
    "    # Extract the labels of the k pairs.\n",
    "    k_labels = ...\n",
    "    # Count the number of +1 predictions and -1 predictions.\n",
    "    pos1_in_k_labels = 0\n",
    "    neg1_in_k_labels = 0\n",
    "    for label in k_labels:\n",
    "        if label == +1:\n",
    "            pos1_in_k_labels += 1\n",
    "        elif label == -1:\n",
    "            neg1_in_k_labels += 1\n",
    "    # Make the prediction based on counts.\n",
    "    if pos1_in_k_labels > neg1_in_k_labels:\n",
    "        y_pred = +1\n",
    "    else:\n",
    "        y_pred = -1\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Judge function: 1(a != b). It supports scalar, vector and matrix.\n",
    "def judge(a, b):\n",
    "    return np.array(a != b).astype(np.float32)\n",
    "\n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, knn_classifier):\n",
    "    e = 0\n",
    "    n = len(X)\n",
    "    for (xi, yi) in zip(X, Y):\n",
    "        e += judge(yi, knn_classifier(xi))\n",
    "    e = 1.0 * e / n\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c699ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "27c699ed"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Visualize the results\n",
    "\n",
    "Please complete the following codes to visualize the decision boundary of the perceptron model. You may use the `vis` function defined above.\n",
    ".\n",
    "\n",
    "You should only insert your code in the `...` part.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7ca8f",
   "metadata": {
    "id": "c4a7ca8f",
    "outputId": "35e20b40-8a9f-4191-b16e-6dec86b281ed",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "opt_val_error = 1.0\n",
    "opt_k = None\n",
    "\n",
    "# Try different k(s).\n",
    "for k in [1,2,3]:\n",
    "    # Visualize\n",
    "    #   1. Validation set (as points).\n",
    "    #   2. Decision boundary from training set (as background).\n",
    "    print(\"k={}\".format(k))\n",
    "    # Create a k-NN classifier with training set.\n",
    "    knn_classifier = partial(f_knn, X_train=X_train, Y_train=Y_train, k=k)\n",
    "    # Visualization.\n",
    "    ...\n",
    "    # Calculate validation error.\n",
    "    val_error = calc_error(X_val, Y_val, knn_classifier)\n",
    "    print(\"Validation error: {}\\n\".format(val_error))\n",
    "    if val_error < opt_val_error:\n",
    "        opt_val_error = val_error\n",
    "        opt_k = k\n",
    "        opt_knn_classifier = knn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d356bc7",
   "metadata": {
    "id": "5d356bc7",
    "outputId": "a89b8800-60f9-4030-e2a9-a9813f8ff754",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Best k={}\".format(opt_k))\n",
    "test_error = calc_error(X_test, Y_test, opt_knn_classifier)\n",
    "...\n",
    "print(\"Test error: {}\".format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc3fcd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "efbc3fcd"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 6 Decision Tree (Bonus)\n",
    "\n",
    "In this problem, you need to implement the decision tree algorithm and apply it to the binary classification. Here we use the Ionosphere dataset $S=\\{(\\mathbf{x}_i, y_i)\\}$ where each feature vector $\\mathbf{x} \\in \\mathbb{R}^{34}$ and label $y \\in \\{-1, +1\\}$. You are allowed to use the functions from `scikit-learn` in this question.\n",
    "\n",
    "- Load the Ionosphere dataset. The dataset $S$ is split to two subsets: The training set $S_\\text{training}$ and the test set $S_\\text{test}$. In the code, we use `X_train`, `Y_train` for the feature vectors and labels of the training set respectively. Similar notations are also used for the test set.\n",
    "\n",
    "- Train the decision tree classifier with the **entropy criterion**. In the decision tree, there is a hyper-parameter $D$ which controls the maximum depth. You would need to perform a grid search on the following list of $D$:\n",
    "$$D \\in \\{1,2,3,4,5\\}$$\n",
    "\n",
    "For each $D$, you need to form a decision tree classifier with the training set $S_\\text{training}$. Specifically, you need to conduct a **10-fold** cross-validation on $S_\\text{training}$ and calculate the cross-validation error $\\bar{e}$ (i.e. average validation error over the splits in cross-validation). We aim to obtain the best hyper-parameter $D^*$ corresponding to the **minimum cross-validation error** $\\bar{e}^*$ among all $D\\text{s}$.\n",
    "\n",
    "\n",
    "- Use the obtained classifier corresponding to the best hyper-parameter $D^*$ to calculate the test error $e_\\text{test}$ on test set $S_\\text{test}$.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411f4eb",
   "metadata": {
    "id": "6411f4eb"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21791e47",
   "metadata": {
    "id": "21791e47"
   },
   "source": [
    "### Load the Ionosphere dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831f420",
   "metadata": {
    "id": "8831f420",
    "outputId": "26f515ac-88f8-430f-d51d-419883dfbcc4"
   },
   "outputs": [],
   "source": [
    "# Ionosphere dataset.\n",
    "X_and_Y = np.load('ionosphere.npy').astype(np.float32) # Load data from file.\n",
    "\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data.\n",
    "X = X_and_Y[:, 0:-1]            # First column to second last column: Features.\n",
    "Y = X_and_Y[:, -1]              # Last column: Labels.\n",
    "Y[Y==0] = -1                    # Convert labels from {0, 1} to {-1, 1}.\n",
    "\n",
    "print(X.shape)      # (351, 34)\n",
    "print(Y.shape)      # (351,)\n",
    "print(X_and_Y[0])\n",
    "# The result should be:\n",
    "# [ 1.       0.      -0.205    0.2875   0.23     0.1      0.2825   0.3175\n",
    "#   0.3225   0.35     0.36285 -0.34617  0.0925   0.275   -0.095    0.21\n",
    "#  -0.0875   0.235   -0.34187  0.31408 -0.48    -0.08     0.29908  0.33176\n",
    "#  -0.58    -0.24     0.3219  -0.28475 -0.47     0.185   -0.27104 -0.31228\n",
    "#   0.40445  0.0305   1.     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2222d",
   "metadata": {
    "id": "16a2222d",
    "outputId": "344161e5-e1fe-4594-8a75-44ad250dc7e7"
   },
   "outputs": [],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X\n",
    "Y_shuffled = Y\n",
    "X_train = X_shuffled[:200]          # Shape: (200, 34)\n",
    "Y_train = Y_shuffled[:200]          # Shape: (200,)\n",
    "X_test = X_shuffled[200:]           # Shape: (151,4)\n",
    "Y_test = Y_shuffled[200:]           # Shape: (151,)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24aa14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4e24aa14"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Decision Tree Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e4113f",
   "metadata": {
    "id": "08e4113f",
    "outputId": "2a557812-719c-4b3a-ebe5-b3a98b6d4f80",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform grid search for best max depth.\n",
    "\n",
    "# 1. Create a decision tree classifier.\n",
    "#    Hint: You can use tree.DecisionTreeClassifier()\n",
    "#          We use \"entropy\" as the criterion. The random state should be\n",
    "#          set to 1 for consistent results. Other options are left at default.\n",
    "estimator = ...\n",
    "# 2. Create a grid searcher with cross-validation.\n",
    "D_list = [1, 2, 3, 4, 5]\n",
    "param_grid = {'max_depth': D_list}\n",
    "#    Hint: You can use GridSearchCV()\n",
    "#          Please set a 10-fold cross-validation.\n",
    "grid_search = ...\n",
    "# 3. Use the grid searcher to fit the training set.\n",
    "#    - This grid searcher will try every max depth in the list.\n",
    "#    - For each max depth, a cross-validation is applied to the training set,\n",
    "#      that is, it creates several (training subset, validation subset) pairs.\n",
    "#      Note: Sometimes the validation subset is called as \"test\" subset, but it\n",
    "#            is not the subset of real test set.\n",
    "#        - For each pair, a decision tree classifier will be trained on the\n",
    "#          training subset and evaluated on validation subset.\n",
    "#        - The average validation scores will be kept.\n",
    "#\n",
    "#    Hint: You can simply use .fit() function of the grid searcher.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6496a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2eb6496a"
   },
   "source": [
    "### Visualize the Results\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f85ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "983f85ac",
    "outputId": "8de0315d-a5f9-4c61-8f77-a077327b831f"
   },
   "outputs": [],
   "source": [
    "# Draw heatmaps for result of grid search.\n",
    "def draw_heatmap(errors, D_list, title):\n",
    "    plt.figure(figsize = (2,4))\n",
    "    ax = sns.heatmap(errors, annot=True, fmt='.3f', yticklabels=D_list, xticklabels=[])\n",
    "    ax.collections[0].colorbar.set_label('error')\n",
    "    ax.set(ylabel='max depth D')\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Draw heatmaps of cross-validation errors (in cross-validation).\n",
    "# Hint: You can use .cv_results['mean_test_score'] to obtain\n",
    "#       cross-validation accuracy (that is, average validation accuracy over\n",
    "#       different splits in the cross-validation). You need to convert it\n",
    "#       to the error.\n",
    "#       Note that you need to reshape the results to shape (?, 1), which is\n",
    "#       needed by draw_heatmap().\n",
    "cross_val_errors = 1 - grid_search.cv_results_['mean_test_score'].reshape(-1,1)\n",
    "draw_heatmap(cross_val_errors, D_list, title='cross-validation error w.r.t D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2eaff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "19b2eaff",
    "outputId": "9ae74016-58d5-40a7-84f5-99da27f06956"
   },
   "outputs": [],
   "source": [
    "# Show the best max depth.\n",
    "# Hint: You can use the .best_params_ of the grid searcher\n",
    "#       to obtain the best parameter(s).\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "print(\"Best max depth D: {}\".format(best_max_depth))\n",
    "\n",
    "# Calculate the test error.\n",
    "# Hint: You can use .best_estimator_.predict() to make predictions.\n",
    "test_error = 1 - sum(grid_search.best_estimator_.predict(X_test) == Y_test) / len(X_test)\n",
    "print(\"Test error: {}\".format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32bc4a-4c9a-4993-96da-6cf39ae23b9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cc32bc4a-4c9a-4993-96da-6cf39ae23b9b"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# END of A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3419d",
   "metadata": {
    "id": "45e3419d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66f6d030",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please make sure to see the output of the gradescope autograder. You are responsible for waiting and ensuring that the autograder is executing normally for your submission. Please create a Piazza post if you see errors in autograder execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a734f6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True, run_tests=True, files=['requirements.txt', 'ionosphere.npy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3708ab",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cogs118a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "otter": {
   "OK_FORMAT": false,
   "tests": {
    "Q1_1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1_1\"\npoints = 10\n\n@test_case(points=None, hidden=False)\ndef check_grad_L_W_b_shape(np, grad_L_W_b):\n    _X = np.random.randint(-100, 100, size=(128, 2))\n    _W = np.random.randint(-100, 100, size=(2,))\n    _b = np.random.randint(-10, 10)\n    _Y = np.random.randint(0, 2, size=(128,))\n    _Y[_Y == 0] = -1\n    _C = 1000\n    assert len(grad_L_W_b(_X, _Y, _W, _b, _C)) == 2\n\n",
    "Q3_1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q3_1\"\npoints = 6\n\n",
    "Q5_1": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q5_1\"\npoints = 8\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
